# -*- coding: utf-8 -*-
"""Hypothesis Testing for Autolib electric car-sharing service company.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iwTYKDOwu9NwSKZmQ_k0adhZ-TZq2FGt

#Problem Statement
We have been tasked to understand electric car usage. We will work as a Data Scientist for the Autolib electric car-sharing service company to investigate a claim about the blue cars from the provided Autolib dataset.

In an effort to do this, we need to identify some areas and periods of interest via sampling stating the reason to the choice of method, then perform hypothesis testing with regards to the claim that we will have made.

##Defining the matrics of success
> This project will be considered a success when we are able to perform the following tasks:

>*   Specify the null and alternate hypothesis.
>*   Conduct EDA to uncover underlying patterns within the dataset that can guide the sampling technique.
>*   Perform hypothesis testing and interpret the results.
>*   Provide project summary and conclusions.

#Data Description
The provided dataset is a daily aggregation, by date and postal code, of the number of events on the Autolib network (car-sharing and recharging).
 The dataset and glossary to use for this project can be found here [https://bit.ly/DSCoreAutolibDataset
"""

import pandas as pd
import seaborn as sn
import numpy as np
import matplotlib.pyplot as plt

#loading our data
df = pd.read_csv ('autolib_daily_events_postal_code.csv')
df.head()

df.tail()

df.info()

#checing for null values
df.isnull().sum()

# Since we are interested in learning more about blue cars only
# we can drop all other unnecessary columns

df = df.drop(df.columns[7:], axis=1)
df.head(5)

# Renaming columns 

df.columns = ['postal_code', 'date', 'daily_data_points', 'day_of_week', 'day_type', 'total_bluecars_taken', 
                'total_bluecars_returned']
df.head()

# Checking for duplicate values 

df.duplicated().any()

# Change Date column to datetime type
df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')

# Creating a new 'month' attribute

df['month'] = df['date'].dt.month

# Replacing day_of week code with actual names

encode = {"day_of_week" : {0:"monday", 1:"tuesday", 2:"wednesday", 3:"thursday",
                           4:"friday", 5:"saturday", 6:"sunday"}, "month" : {1:"January",
                          2:"February", 3:"March", 4:"April", 5:"May", 6:"June"}}

df.replace(encode, inplace = True)
df.head()

# Checking for Outliers

fig, ax = plt.subplots(1,2, figsize=(20,8))
fig.suptitle('Boxplots')
sn.boxplot(data=df, y = "total_bluecars_taken", ax=ax[0])
ax[0].set_title('Box Plot of Blue Cars Taken')
sn.boxplot(data=df, y = "total_bluecars_returned", ax=ax[1])
ax[1].set_title('Box Plot of Blue Cars Returned')


plt.show()

"""There are quite alot of  'outliers' in the dataset for the number of blue cars taken and returned. I choose not to remove the outliers since there is no evidence of the being erraneous records """

# Checking for Anomalies
# 'blue cars taken' variable

q1_taken = df['total_bluecars_taken'].quantile(.25)
q3_taken = df['total_bluecars_taken'].quantile(.75)

iqr_taken = q3_taken - q1_taken

# 'blue cars returned' variable
q1_returned = df['total_bluecars_returned'].quantile(.25)
q3_returned = df['total_bluecars_returned'].quantile(.75)

iqr_returned = q3_returned - q1_returned

# 'available data points' variable
q1_points = df['daily_data_points'].quantile(.25)
q3_points = df['daily_data_points'].quantile(.75)

iqr_points = q3_points - q1_points

print(iqr_taken, iqr_returned, iqr_points)

"""The results show that the blue cars taken and blue cars returned attributes both have 115 records that are not within the Quantile 2 of the records in those columns.only one record is on the 3rd quantile

#Univariate  Analysis
"""

# frequency table for the postal code attribute

df.postal_code.value_counts()

# How often does each day of the week appear?

df.day_of_week.value_counts()

# How often does each day type appear?

df.day_type.value_counts()

"""###measuring central tendacy

"""

#mean of total_bluecars_taken
df.total_bluecars_taken.mean()

#mean of total_bluecars_returned
df.total_bluecars_returned .mean()

#mode of BlueCars_taken_sum
df.total_bluecars_taken.mode()

#mode of total_bluecars_returned
df.total_bluecars_returned .mode()

# median of BlueCars_taken_sum
df.total_bluecars_taken.median()

#median of total_bluecars_returned
df.total_bluecars_returned .median()

"""#Measures of Dispersion"""

# standard deviation for total_bluecars_taken
df.total_bluecars_taken.std()

# standard deviation for total_bluecars_returned
df.total_bluecars_returned.std()

#variance of total_bluecars_taken
df.total_bluecars_taken.var()

# var deviation for total_bluecars_returned
df.total_bluecars_returned.var()

#skewness for total_bluecars_taken
df.total_bluecars_taken.skew()

# skewness   for total_bluecars_returned
df.total_bluecars_returned.skew()

#kurtosis for total_bluecars_taken
df.total_bluecars_taken.kurt()

# kurtosis   for total_bluecars_returned
df.total_bluecars_returned.kurt()

# plotting histograms to show the distribution of blue cars taken and returned

fig,ax=plt.subplots(1,2,figsize=(20,10))
df['total_bluecars_taken'].plot.hist(ax=ax[0],bins=5,edgecolor='black',color='maroon')
ax[0].set_title('Distribution of Blue Cars Taken')
x1=list(range(0,85,5))
ax[0].set_xticks(x1)
df['total_bluecars_returned'].plot.hist(ax=ax[1],color='teal',bins=5,edgecolor='black')
ax[1].set_title('Distribution of Blue Cars Returned')
x2=list(range(0,20,2))
ax[1].set_xticks(x2)
plt.show()

"""From the above distribution we can see that both plots are  not normaly distributed but are skewed to the left"""

# Frequency distribution plots

col_names = ['total_bluecars_taken','total_bluecars_returned']
fig, ax = plt.subplots(len(col_names), figsize=(10,20))

for i, col_val in enumerate(col_names):

    sn.distplot(df[col_val], hist=True, ax=ax[i])
    ax[i].set_title('Frequency distribution of '+col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()

plt.figure(figsize=(16, 6))
df.BlueCars_taken_sum.hist() 
plt.xlabel('BlueCars_taken_sum')

"""From the above distribution we can see that BlueCars_taken_sum is not normaly distributed with an average of 125

#Bivariate Analysis
"""

# Ploting the bivariate summaries 

sn.pairplot(df)
plt.show()

# Plotting the Pearson correlation coefficient among numeric variables


sn.heatmap(df.corr(),annot=True)
plt.show()

# scatter plot of blue cars taken and blue cars returned

sn.lmplot('total_bluecars_taken', "total_bluecars_returned", df, scatter_kws={'marker':'o', 'color': 'indianred'}, 
           line_kws={'linewidth':1,'color':'blue'}, height = 4, aspect = 2)

plt.title("Correlation of Blue Cars Taken and Returned")
plt.xlabel('Blue Cars Taken')
plt.ylabel("Blue Cars Returned")

"""We can see from the plot that total_bluecars_taken is directtly propotional to total_bluecars_returned"""

# Bar chart showing total number of blue cars picked up by day of week

df.groupby('day_of_week')['total_bluecars_taken'].sum().plot(kind='bar', figsize=(10,5))
plt.xlabel("Day")
plt.xticks(rotation=45)
plt.ylabel("Number of Blue Cars Taken")
plt.title("Pick Up Rates By Day of Week")
plt.show()

"""The pick up rate for blue cars is higher on weekends """

# Bar chart showing total number of blue cars returned   by day of week

df.groupby('day_of_week')['total_bluecars_returned'].sum().plot(kind='bar', figsize=(10,5))
plt.xlabel("Day")
plt.xticks(rotation=45)
plt.ylabel("Number of Blue Cars Returned")
plt.title("Return Rates By Day of Week")
plt.show()

"""The return rate for blue cars is higher on weekends"""

# Bar chart showing total number of blue cars picked up by month

df.groupby('month')['total_bluecars_taken'].sum().plot(kind='bar', figsize=(10,5))
plt.xlabel("Day")
plt.xticks(rotation=45)
plt.ylabel("Number of Blue Cars Taken")
plt.title("Pick Up Rates By Day of Week")
plt.show()

# Bar chart showing total number of blue cars returned   by month

df.groupby('month')['total_bluecars_returned'].sum().plot(kind='bar', figsize=(10,5))
plt.xlabel("Day")
plt.xticks(rotation=45)
plt.ylabel("Number of Blue Cars Returned")
plt.title("Return Rates By Day of Week")
plt.show()

"""#Hypothesis Testing
### Specifying the question
Null Hypothesis: For both Paris and Essonne the average number of blue cars picked up during weekends is the same.

Ho : μ1 = μ2 (where μ1 is the mean for Paris and μ2 is the mean for Essonne)

Alternate Hypothesis: The average number of blue cars picked up during the weekend is not the same for Paris and Essonne.

Ha : μ1 ≠ μ2
"""

# Checking all unique postal codes in the dataset

df['postal_code'].unique()

# Coding all postal codes with the city they represent

# create a list of our conditions
conditions = [
    (df['postal_code'] >= 75000) & (df['postal_code'] <= 75999),
    (df['postal_code'] >= 78000) & (df['postal_code'] <= 78999),
    (df['postal_code'] >= 91000) & (df['postal_code'] <= 91999),
    (df['postal_code'] >= 92000) & (df['postal_code'] <= 92999),
    (df['postal_code'] >= 93000) & (df['postal_code'] <= 93999),
    (df['postal_code'] >= 94000) & (df['postal_code'] <= 94999),
    (df['postal_code'] >= 95000) & (df['postal_code'] <= 95999),
    ]

# create a list of the values we want to assign for each condition
values = ['Paris', 'Yvelines', 'Essonne', 'Hauts-de-Seine', 'Seine-Saint_Denis', 'Val-de-Marne', 'Val-dOise']

# create a new column and use np.select to assign values to it using our lists as arguments
df['city'] = np.select(conditions, values)

# display updated DataFrame
df.head()



"""##Sampling approach

Since we are comparing  samples from two different cities  stratified random sampling is the best sampling technique to use.
"""

# Creating a new dataframe with only Paris and Hauts-de-Seine
# and only weekend records which is the target records

df_sample= df.loc[df['city'].isin(['Paris','Essonne']) & df['day_type'].isin(['weekend'])]
df_sample

#previewing top sample data 
df_sample.head()

# Checking the distribution of usage/records by city

df_sample['city'].value_counts()

# We will choose a sample size of 10% of all target records

sample = df_sample.groupby('city', group_keys=False).apply(lambda grouped_subset : grouped_subset.sample(frac=0.1))

# proportion of the stratified sample
print(sample['city'].value_counts())

# Check the stratified output
print(sample)

# Performing normality test on the sample selected

from statsmodels.graphics.gofplots import qqplot

# q-q plot

qqplot(sample['total_bluecars_taken'], line='s')
plt.show()

"""###For  hypothesis testing, we will use a two sample z-test and p-value to either reject or accept the null hypothesis since aour sample is greater than 30."""

#define the two samples 
Paris = sample[(sample['city'] == 'Paris')]
Essonne = sample[(sample['city'] == 'Essonne')]

print(Paris)
print(Essonne)

# Next, we calculate the z-score and p value

from scipy import stats
from statsmodels.stats import weightstats as stests

ztest ,pval1 = stests.ztest(x1=Paris['total_bluecars_taken'], x2=Essonne['total_bluecars_taken'], value=0,alternative='two-sided')
print(float(pval1))
if pval1<0.025:
    print("Reject Null Hypothesis")
else:
    print("Accept Null Hypothesis")

print(ztest)

"""We can see that the p-value from the  sample z-test is less than 0.05 therefore we reject the null hypothesis

#Hypothesis Testing Results and Interpretation

We calculated the p value of the two sample z test and found that the p values is 8.376453894258129e-10 which is much less than 0.05. Therefore, we reject the null hypothesis. This means that the average number of blue cars picked up on weekends is not the same for Paris and that for Essonne.

##Summary and Conclusions

We have successfully defined the null and alternate hypothesis, executed the sampling technique and carried out hypothesis testing which led to the rejection of the null hypothesis. We concluded that the average number of blue cars picked up in Essonne is not the same as that in Paris.
"""

